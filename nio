df_seg = df_raw.filter(col("c0") == seg_code)

# fast empty check
if df_seg.limit(1).count() == 0:
    print(f"No rows for {seg_key} ({seg_code}); skipping.")
    continue

df_seg = apply_field_count(df_seg, field_count, max_schema_cols)

# Convert to selected schema once
df_seg = df_seg.select([
    col(f"c{i}").alias(schema_cols[i]) if f"c{i}" in df_seg.columns else lit(None).alias(schema_cols[i])
    for i in range(len(schema_cols))
])

# Dedup - cache for reuse
if dedup_keys:
    df_seg = dedup_latest(df_seg, dedup_keys, dedup_order).cache()

# Rollup
if rollup_keys and rollup_sums:
    df_seg = rollup(df_seg, rollup_keys, rollup_sums)

df_seg = df_seg.withColumn("BATCH_ID", lit(load_uuid)) \
               .withColumn("CREATE_ID", lit("PYSPARK")) \
               .withColumn("CREATE_DT", current_timestamp())

# Write to Oracle (optimized JDBC)
df_seg.write \
    .format("jdbc") \
    .option("url", JDBC_URL) \
    .option("dbtable", target_table) \
    .option("user", ORACLE_USER) \
    .option("password", ORACLE_PASS) \
    .option("driver", JDBC_DRIVER) \
    .option("batchsize", "10000") \
    .option("numPartitions", "4") \
    .mode("append") \
    .save()

# Fast count query
target_count = spark.read.format("jdbc") \
    .option("url", JDBC_URL) \
    .option("query", f"SELECT COUNT(1) CNT FROM {target_table}") \
    .option("user", ORACLE_USER) \
    .option("password", ORACLE_PASS) \
    .load().collect()[0]["CNT"]

print(f"Target Count: {target_count}")
