from pyspark.sql import Window
from pyspark.sql.functions import col, when, sum as _sum, row_number, lit, current_timestamp
import uuid

def apply_field_count(df, exp_count):
    cnt_expr = sum([when(col(f"c{i}").isNotNull(), 1).otherwise(0) for i in range(max_schema_cols)])
    return df.withColumn("field_count", cnt_expr).filter(col("field_count") == exp_count).drop("field_count")

def dedup_latest(df, keys, order_cols):
    w = Window.partitionBy([col(k) for k in keys]).orderBy(*[col(c).desc() for c in order_cols])
    return df.withColumn("rn", row_number().over(w)).filter(col("rn") == 1).drop("rn")

def rollup(df, keys, sum_cols):
    return df.groupBy(keys).agg(*[_sum(col(c)).alias(c) for c in sum_cols])

if df_raw.rdd.isEmpty():
    print("No data rows found; nothing to load.")
else:
    for seg_code, seg_key in segment_map.items():
        seg_cfg = cfg_root.get(seg_key)
        if not seg_cfg:
            continue

        schema_cols  = seg_cfg["schema"]
        target_table = seg_cfg["target_table"]
        field_count  = seg_cfg["field_count"]
        dedup_keys   = seg_cfg["dedup_keys"]
        dedup_order  = seg_cfg.get("dedup_order", ["LST_UPD_DT", "LST_UPD_TIME"])
        rollup_keys  = seg_cfg.get("rollup_keys", dedup_keys)
        rollup_sums  = seg_cfg.get("rollup_sums", [c for c in schema_cols if c not in rollup_keys])

        df_seg = df_raw.filter(col("c0") == seg_code)
        if df_seg.rdd.isEmpty():
            print(f"No rows for {seg_key} ({seg_code}); skipping.")
            continue

        df_seg = apply_field_count(df_seg, field_count)
        if df_seg.rdd.isEmpty():
            print(f"All rows dropped by field_count for {seg_key}; skipping.")
            continue

        # align to schema by position
        df_seg = df_seg.select([col(f"c{i}").alias(schema_cols[i]) for i in range(len(schema_cols))])

        if dedup_keys:
            df_seg = dedup_latest(df_seg, dedup_keys, dedup_order)
        if rollup_keys and rollup_sums:
            df_seg = rollup(df_seg, rollup_keys, rollup_sums)
        if "load_status" in df_seg.columns:
            df_seg = df_seg.filter((col("load_status").isNull()) | (col("load_status") != "RGN FILTERED OUT"))

        # add audit/meta columns
        load_uuid = str(uuid.uuid4())
        df_seg = (
            df_seg
            .withColumn("BATCH_ID", lit(load_uuid))
            .withColumn("CREATE_ID", lit("PYSPARK"))
            .withColumn("CREATE_DT", current_timestamp())
        )

        # final column order: schema_cols plus audit cols at the end
        final_cols = schema_cols + ["BATCH_ID", "CREATE_ID", "CREATE_DT"]
        df_seg = df_seg.select([col(c) for c in final_cols])

        # coalesce to reduce write overhead
        df_seg = df_seg.coalesce(4)

        print(f"Write to Target DB Table: {target_table}")
        write_to_oracle(env, df_seg, target_table, merge_keys=dedup_keys)
        print(f"Data successfully written to {target_table} in Oracle.")

        tgt_count = get_table_count(spark, env, target_table, source_system)
        print(f"Target Count for {target_table}: {int(tgt_count)}")
